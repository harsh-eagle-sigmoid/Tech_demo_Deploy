"""
Main result validation orchestrator - executes and compares query outputs
"""
from typing import Dict, Optional
from dataclasses import dataclass
from loguru import logger
from .query_executor import QueryExecutor, ExecutionResult
from .result_comparator import ResultComparator, ComparisonResult
from .llm_output_judge import LLMOutputJudge


@dataclass
class ValidationResult:
    """Complete validation result with scoring"""
    score: float  # 0.0 to 1.0
    confidence: str  # HIGH, MEDIUM, LOW
    execution_success: bool
    schema_match: bool
    row_count_match: bool
    content_match_rate: float
    generated_execution_time_ms: Optional[float] = None
    gt_execution_time_ms: Optional[float] = None
    error: Optional[str] = None
    details: Optional[Dict] = None


class ResultValidator:
    """
    Validates SQL query outputs by executing and comparing against ground truth.
    Integrates with existing structural and semantic evaluation layers.
    """

    def __init__(
        self,
        timeout_seconds: int = 10,
        max_rows: int = 10000,
        epsilon: float = 0.0001
    ):
        """
        Args:
            timeout_seconds: Query execution timeout
            max_rows: Max rows to fetch per query
            epsilon: Float comparison tolerance
        """
        self.executor = QueryExecutor(timeout_seconds, max_rows)
        self.comparator = ResultComparator(epsilon)
        self.llm_judge = LLMOutputJudge()

    def validate(
        self,
        generated_sql: str,
        ground_truth_sql: str,
        db_url: str,
        gt_match_confidence: str = "HIGH"
    ) -> ValidationResult:
        """
        Validate generated SQL by comparing its output to ground truth.

        Args:
            generated_sql: SQL generated by the agent
            ground_truth_sql: Reference SQL from ground truth
            db_url: Database connection URL
            gt_match_confidence: Confidence of GT match (HIGH/MEDIUM/LOW)

        Returns:
            ValidationResult with score and detailed breakdown
        """
        logger.info(f"Validating query output (GT confidence: {gt_match_confidence})")

        # Execute generated SQL
        gen_result = self.executor.execute(generated_sql, db_url)
        if not gen_result.success:
            logger.error(f"Generated SQL failed: {gen_result.error}")
            return ValidationResult(
                score=0.0,
                confidence=gt_match_confidence,
                execution_success=False,
                schema_match=False,
                row_count_match=False,
                content_match_rate=0.0,
                error=f"Generated SQL execution failed: {gen_result.error}",
                details={'generated_error': gen_result.error}
            )

        # Execute ground truth SQL
        gt_result = self.executor.execute(ground_truth_sql, db_url)
        if not gt_result.success:
            logger.error(f"Ground truth SQL failed: {gt_result.error}")
            return ValidationResult(
                score=0.0,
                confidence=gt_match_confidence,
                execution_success=False,
                schema_match=False,
                row_count_match=False,
                content_match_rate=0.0,
                error=f"Ground truth SQL execution failed: {gt_result.error}",
                details={'gt_error': gt_result.error}
            )

        # Compare results
        comparison = self.comparator.compare(
            result1_columns=gen_result.columns,
            result1_rows=gen_result.rows,
            result2_columns=gt_result.columns,
            result2_rows=gt_result.rows,
            sql1=generated_sql,
            sql2=ground_truth_sql
        )

        # Apply confidence weighting to score
        final_score = self._apply_confidence_weight(
            comparison.score,
            gt_match_confidence
        )

        logger.info(
            f"Validation complete: score={final_score:.2f}, "
            f"schema_match={comparison.schema_match}, "
            f"content_match={comparison.content_match_rate:.2f}"
        )

        # Store sample output for display (first 10 rows)
        sample_size = min(10, len(gen_result.rows) if gen_result.rows else 0)
        output_sample = {
            'columns': gen_result.columns,
            'rows': [list(row) for row in gen_result.rows[:sample_size]] if gen_result.rows else []
        }

        return ValidationResult(
            score=final_score,
            confidence=gt_match_confidence,
            execution_success=True,
            schema_match=comparison.schema_match,
            row_count_match=comparison.row_count_match,
            content_match_rate=comparison.content_match_rate,
            generated_execution_time_ms=gen_result.execution_time_ms,
            gt_execution_time_ms=gt_result.execution_time_ms,
            error=None,
            details={
                'comparison': comparison.details,
                'gen_row_count': gen_result.row_count,
                'gt_row_count': gt_result.row_count,
                'output_sample': output_sample  # NEW: Store sample for display
            }
        )

    def validate_syntax_only(self, generated_sql: str, db_url: str) -> ValidationResult:
        """
        Partial validation when no ground truth available.
        Only checks if query executes successfully.

        Args:
            generated_sql: SQL to validate
            db_url: Database connection URL

        Returns:
            ValidationResult with partial score
        """
        logger.info("Performing syntax-only validation (no GT available)")

        result = self.executor.execute(generated_sql, db_url)

        if result.success:
            # Query executed successfully
            # Give partial credit (60%)
            return ValidationResult(
                score=0.6,
                confidence="LOW",
                execution_success=True,
                schema_match=True,  # Can't verify, assume true
                row_count_match=True,  # Can't verify, assume true
                content_match_rate=0.0,  # Unknown
                generated_execution_time_ms=result.execution_time_ms,
                error=None,
                details={
                    'validation_type': 'syntax_only',
                    'row_count': result.row_count,
                    'column_count': len(result.columns) if result.columns else 0
                }
            )
        else:
            # Query failed
            return ValidationResult(
                score=0.0,
                confidence="LOW",
                execution_success=False,
                schema_match=False,
                row_count_match=False,
                content_match_rate=0.0,
                error=f"Query execution failed: {result.error}",
                details={'error': result.error}
            )

    def validate_with_llm(
        self,
        query_text: str,
        generated_sql: str,
        db_url: str
    ) -> ValidationResult:
        """
        Enhanced validation without ground truth using LLM-based reasoning.
        Combines execution check + LLM output evaluation.

        Args:
            query_text: User's natural language query
            generated_sql: SQL generated by the agent
            db_url: Database connection URL

        Returns:
            ValidationResult with LLM-enhanced score
        """
        logger.info(f"Performing LLM-based output validation (no GT available)")

        # Tier 1: Execute query
        result = self.executor.execute(generated_sql, db_url)

        if not result.success:
            # Execution failed - score 0
            logger.error(f"Query execution failed: {result.error}")
            return ValidationResult(
                score=0.0,
                confidence="MEDIUM",
                execution_success=False,
                schema_match=False,
                row_count_match=False,
                content_match_rate=0.0,
                error=f"Query execution failed: {result.error}",
                details={
                    'validation_type': 'llm_enhanced',
                    'execution_error': result.error,
                    'tier1_execution': 0.0,
                    'tier2_llm_reasoning': 0.0
                }
            )

        # Store sample output for display
        sample_size = min(10, len(result.rows) if result.rows else 0)
        output_sample = {
            'columns': result.columns,
            'rows': [list(row) for row in result.rows[:sample_size]] if result.rows else []
        }

        # Tier 2: LLM-based output reasoning
        try:
            llm_scores = self.llm_judge.evaluate_output(
                query_text=query_text,
                sql=generated_sql,
                columns=result.columns or [],
                rows=result.rows[:5] if result.rows else [],  # Sample first 5 rows for LLM
                row_count=result.row_count,
                execution_time_ms=result.execution_time_ms or 0.0
            )

            # Combined scoring:
            # - Execution success: 30% (Tier 1)
            # - LLM reasoning: 70% (Tier 2)
            execution_score = 1.0  # Passed execution
            llm_score = llm_scores.overall

            final_score = (0.30 * execution_score) + (0.70 * llm_score)

            logger.info(
                f"LLM validation complete: exec={execution_score:.2f}, "
                f"llm={llm_score:.2f}, final={final_score:.2f}"
            )

            return ValidationResult(
                score=final_score,
                confidence="MEDIUM",
                execution_success=True,
                schema_match=True,  # Assume true since execution succeeded
                row_count_match=True,  # Can't verify without GT
                content_match_rate=llm_scores.correctness,  # Use LLM correctness as proxy
                generated_execution_time_ms=result.execution_time_ms,
                error=None,
                details={
                    'validation_type': 'llm_enhanced',
                    'tier1_execution': execution_score,
                    'tier2_llm_reasoning': llm_score,
                    'llm_correctness': llm_scores.correctness,
                    'llm_completeness': llm_scores.completeness,
                    'llm_quality': llm_scores.quality,
                    'llm_reasoning': llm_scores.reasoning,
                    'gen_row_count': result.row_count,  # Match frontend expectation
                    'column_count': len(result.columns) if result.columns else 0,
                    'execution_time_ms': result.execution_time_ms,
                    'output_sample': output_sample
                }
            )

        except Exception as e:
            logger.error(f"LLM validation failed, falling back to execution-only: {e}")
            # Fallback to basic execution validation
            return ValidationResult(
                score=0.6,  # Partial credit for successful execution
                confidence="LOW",
                execution_success=True,
                schema_match=True,
                row_count_match=True,
                content_match_rate=0.0,
                generated_execution_time_ms=result.execution_time_ms,
                error=None,
                details={
                    'validation_type': 'execution_only_fallback',
                    'llm_error': str(e),
                    'gen_row_count': result.row_count,  # Match frontend expectation
                    'column_count': len(result.columns) if result.columns else 0,
                    'execution_time_ms': result.execution_time_ms,
                    'output_sample': output_sample
                }
            )

    def validate_with_gt_output(
        self,
        query_text: str,
        generated_sql: str,
        gt_expected_output: Dict,
        db_url: str
    ) -> ValidationResult:
        """
        Validate generated SQL by comparing its output against GT expected output.
        This is MORE ACCURATE than SQL structure comparison.

        Args:
            query_text: User's natural language query
            generated_sql: SQL generated by the agent
            gt_expected_output: Ground truth expected output with columns, rows, row_count
            db_url: Database connection URL

        Returns:
            ValidationResult with output comparison score
        """
        logger.info(f"Validating with GT expected output comparison")

        # Execute generated SQL
        gen_result = self.executor.execute(generated_sql, db_url)

        if not gen_result.success:
            logger.error(f"Generated query execution failed: {gen_result.error}")
            return ValidationResult(
                score=0.0,
                confidence="HIGH",  # High confidence in failure
                execution_success=False,
                schema_match=False,
                row_count_match=False,
                content_match_rate=0.0,
                generated_execution_time_ms=None,
                error=f"Query execution failed: {gen_result.error}",
                details={
                    'validation_type': 'gt_output_comparison',
                    'execution_error': gen_result.error
                }
            )

        # Compare generated output against GT expected output
        gt_columns = gt_expected_output.get("columns", [])
        gt_row_count = gt_expected_output.get("row_count", 0)
        gt_sample_rows = gt_expected_output.get("sample_rows", [])

        # Convert GT sample rows to tuples for comparison
        gt_rows_tuples = [tuple(row) for row in gt_sample_rows]

        # Create a fake ExecutionResult for GT to use existing comparator
        from .query_executor import ExecutionResult
        gt_result = ExecutionResult(
            success=True,
            columns=gt_columns,
            rows=gt_rows_tuples,
            row_count=gt_row_count,
            execution_time_ms=gt_expected_output.get("execution_time_ms"),
            error=None
        )

        # Compare results
        comparison = self.comparator.compare(gen_result, gt_result)

        # Calculate weighted score
        # Schema: 20%, Row count: 20%, Content: 60%
        score = (
            0.20 * (1.0 if comparison.schema_match else 0.0) +
            0.20 * (1.0 if comparison.row_count_match else 0.5) +
            0.60 * comparison.content_match_rate
        )

        # Determine confidence based on match quality
        if score >= 0.95:
            confidence = "HIGH"
        elif score >= 0.75:
            confidence = "MEDIUM"
        else:
            confidence = "LOW"

        # Store sample output for display
        sample_size = min(10, len(gen_result.rows) if gen_result.rows else 0)
        output_sample = {
            'columns': gen_result.columns,
            'rows': [list(row) for row in gen_result.rows[:sample_size]] if gen_result.rows else []
        }

        logger.info(
            f"GT output validation complete: score={score:.2f}, "
            f"schema_match={comparison.schema_match}, "
            f"row_count_match={comparison.row_count_match}, "
            f"content_match={comparison.content_match_rate:.2f}"
        )

        return ValidationResult(
            score=score,
            confidence=confidence,
            execution_success=True,
            schema_match=comparison.schema_match,
            row_count_match=comparison.row_count_match,
            content_match_rate=comparison.content_match_rate,
            generated_execution_time_ms=gen_result.execution_time_ms,
            gt_execution_time_ms=gt_expected_output.get("execution_time_ms"),
            error=None,
            details={
                'validation_type': 'gt_output_comparison',
                'schema_diff': comparison.schema_diff,
                'row_count_generated': gen_result.row_count,
                'row_count_expected': gt_row_count,
                'content_differences': comparison.differing_rows,
                'gen_row_count': gen_result.row_count,
                'execution_time_ms': gen_result.execution_time_ms,
                'output_sample': output_sample
            }
        )

    def _apply_confidence_weight(self, score: float, confidence: str) -> float:
        """
        Apply confidence-based weighting to final score.

        HIGH confidence (>90% GT match): Full score
        MEDIUM confidence (75-90% GT match): 80% weight
        LOW confidence (<75% GT match): 50% weight
        """
        if confidence == "HIGH":
            return score
        elif confidence == "MEDIUM":
            return score * 0.8
        elif confidence == "LOW":
            return score * 0.5
        else:
            return score

    def get_performance_comparison(
        self,
        gen_time_ms: Optional[float],
        gt_time_ms: Optional[float]
    ) -> Dict[str, any]:
        """
        Compare execution performance of generated vs GT query.

        Returns:
            Dictionary with performance metrics
        """
        if gen_time_ms is None or gt_time_ms is None:
            return {'comparison': 'unavailable'}

        speedup = gt_time_ms / gen_time_ms if gen_time_ms > 0 else 0
        slowdown = gen_time_ms / gt_time_ms if gt_time_ms > 0 else 0

        if speedup > 1.2:
            performance = "faster"
        elif slowdown > 1.2:
            performance = "slower"
        else:
            performance = "similar"

        return {
            'generated_ms': gen_time_ms,
            'ground_truth_ms': gt_time_ms,
            'speedup': speedup,
            'slowdown': slowdown,
            'performance': performance
        }
