"""
LLM-as-Judge for Query Evaluation
Step 4 of Evaluation Framework
"""
from typing import Dict
from loguru import logger
from agents.llm_client import LLMClient
from config.settings import settings


class LLMJudge:
    """Uses LLM to evaluate if agent response correctly answers the query"""

    def __init__(self):
        """Initialize LLM judge"""
        self.llm = LLMClient(provider=settings.EVALUATOR_LLM_PROVIDER)
        logger.info(f"Initialized LLM Judge with provider: {settings.EVALUATOR_LLM_PROVIDER}")

    def evaluate(
        self,
        user_query: str,
        generated_sql: str,
        ground_truth_sql: str,
        agent_type: str
    ) -> Dict:
        """
        Evaluate if generated SQL correctly answers the user query

        Args:
            user_query: Original natural language query
            generated_sql: SQL generated by agent
            ground_truth_sql: Expected SQL from ground truth
            agent_type: 'spend' or 'demand'

        Returns:
            Dict with evaluation results
        """
        try:
            # Build evaluation prompt
            system_prompt = """You are an expert SQL evaluator. Your task is to determine if the generated SQL query correctly answers the user's question.

Evaluation Criteria:
1. **Correctness**: Does the SQL query retrieve the right data to answer the question?
2. **Completeness**: Does it include all necessary components (filters, aggregations, etc.)?
3. **Logic**: Are the table joins, WHERE conditions, and GROUP BY clauses correct?

Compare the generated SQL with the ground truth SQL. Consider them equivalent if they produce the same result, even if syntax differs slightly.

Return your evaluation in this exact format:
VERDICT: [PASS/FAIL]
CONFIDENCE: [0.0-1.0]
REASONING: [Brief explanation of your decision]"""

            user_prompt = f"""User Query: "{user_query}"

Generated SQL:
{generated_sql}

Ground Truth SQL:
{ground_truth_sql}

Agent Type: {agent_type}

Evaluate if the generated SQL correctly answers the user query."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]

            # Get LLM evaluation
            response = self.llm.generate(
                messages=messages,
                temperature=0.0,  # Deterministic
                max_tokens=300
            )

            # Parse response
            result = self._parse_response(response)

            return result

        except Exception as e:
            logger.error(f"Error in LLM evaluation: {e}")
            return {
                "verdict": "FAIL",
                "confidence": 0.0,
                "reasoning": f"Error during evaluation: {str(e)}",
                "raw_response": str(e)
            }

    def _parse_response(self, response: str) -> Dict:
        """
        Parse LLM response

        Args:
            response: Raw LLM response

        Returns:
            Parsed evaluation dict
        """
        result = {
            "verdict": "FAIL",
            "confidence": 0.5,
            "reasoning": "",
            "raw_response": response
        }

        try:
            lines = response.strip().split('\n')

            for line in lines:
                line = line.strip()

                if line.startswith("VERDICT:"):
                    verdict_str = line.replace("VERDICT:", "").strip().upper()
                    result["verdict"] = verdict_str if verdict_str in ["PASS", "FAIL"] else "FAIL"

                elif line.startswith("CONFIDENCE:"):
                    try:
                        conf_str = line.replace("CONFIDENCE:", "").strip()
                        result["confidence"] = float(conf_str)
                    except:
                        result["confidence"] = 0.5

                elif line.startswith("REASONING:"):
                    result["reasoning"] = line.replace("REASONING:", "").strip()

            # If reasoning is empty, use full response
            if not result["reasoning"]:
                result["reasoning"] = response

        except Exception as e:
            logger.warning(f"Error parsing LLM response: {e}")
            result["reasoning"] = response

        return result
