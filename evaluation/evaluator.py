"""
Main Evaluator - Orchestrates 6-Step Evaluation Pipeline
"""
import json
import psycopg2
from datetime import datetime
from typing import Dict, Optional
from loguru import logger
from config.settings import settings
from evaluation.validators import StructuralValidator
from evaluation.semantic_checker import SemanticChecker
from evaluation.llm_judge import LLMJudge


class Evaluator:
    """Complete evaluation pipeline for agent responses"""

    def __init__(self, agent_type: str):
        """
        Initialize evaluator

        Args:
            agent_type: 'spend' or 'demand'
        """
        self.agent_type = agent_type
        self.schema_name = "spend_data" if agent_type == "spend" else "demand_data"

        # Initialize components
        self.structural_validator = StructuralValidator(self.schema_name)
        self.semantic_checker = SemanticChecker()
        self.llm_judge = LLMJudge()

        logger.info(f"Initialized Evaluator for {agent_type} agent")

    def _get_db_connection(self):
        """Get database connection"""
        return psycopg2.connect(
            host=settings.DB_HOST,
            port=settings.DB_PORT,
            database=settings.DB_NAME,
            user=settings.DB_USER,
            password=settings.DB_PASSWORD
        )

    def preprocess(self, query_text: str, generated_sql: str) -> Dict:
        """
        Step 1: Pre-process query and SQL

        Args:
            query_text: User query
            generated_sql: Generated SQL

        Returns:
            Preprocessed data
        """
        # Clean and normalize
        cleaned_sql = generated_sql.strip()

        # Remove markdown code blocks if present
        if "```" in cleaned_sql:
            cleaned_sql = cleaned_sql.split("```")[1] if cleaned_sql.count("```") >= 2 else cleaned_sql
            cleaned_sql = cleaned_sql.replace("sql", "").strip()

        return {
            "query_text": query_text.strip(),
            "cleaned_sql": cleaned_sql,
            "original_sql": generated_sql
        }

    def evaluate(
        self,
        query_id: str,
        query_text: str,
        generated_sql: str,
        ground_truth_sql: str,
        complexity: str = "unknown"
    ) -> Dict:
        """
        Complete 6-step evaluation pipeline

        Args:
            query_id: Unique query identifier
            query_text: Original user query
            generated_sql: SQL generated by agent
            ground_truth_sql: Expected SQL from ground truth
            complexity: Query complexity (simple/medium/complex)

        Returns:
            Complete evaluation results
        """
        logger.info(f"Evaluating query {query_id}: {query_text[:50]}...")

        result = {
            "query_id": query_id,
            "query_text": query_text,
            "generated_sql": generated_sql,
            "ground_truth_sql": ground_truth_sql,
            "complexity": complexity,
            "agent_type": self.agent_type,
            "timestamp": datetime.now().isoformat(),
            "steps": {},
            "scores": {},
            "final_result": "FAIL",
            "final_score": 0.0,
            "confidence": 0.0
        }

        try:
            # Step 1: Pre-processing
            preprocessed = self.preprocess(query_text, generated_sql)
            result["steps"]["preprocessing"] = preprocessed
            cleaned_sql = preprocessed["cleaned_sql"]

            # Step 2: Structural Validation
            logger.debug(f"Step 2: Structural validation for {query_id}")
            structural_result = self.structural_validator.validate(cleaned_sql)
            result["steps"]["structural_validation"] = structural_result
            result["scores"]["structural"] = structural_result["score"]

            # If structural validation fails badly, skip remaining steps
            if structural_result["score"] == 0.0:
                result["final_result"] = "FAIL"
                result["final_score"] = 0.0
                result["confidence"] = 1.0
                logger.warning(f"Query {query_id} failed structural validation")
                return result

            # Step 3: Semantic Check
            logger.debug(f"Step 3: Semantic check for {query_id}")
            semantic_result = self.semantic_checker.check_semantic_equivalence(
                cleaned_sql,
                ground_truth_sql
            )
            result["steps"]["semantic_check"] = semantic_result
            result["scores"]["semantic"] = semantic_result["similarity_score"]

            # Step 4: LLM as Judge
            logger.debug(f"Step 4: LLM evaluation for {query_id}")
            llm_result = self.llm_judge.evaluate(
                query_text,
                cleaned_sql,
                ground_truth_sql,
                self.agent_type
            )
            result["steps"]["llm_judge"] = llm_result

            # Convert LLM verdict to score
            llm_score = 1.0 if llm_result["verdict"] == "PASS" else 0.0
            result["scores"]["llm"] = llm_score

            # Step 5: Scoring and Decision
            logger.debug(f"Step 5: Final scoring for {query_id}")
            final_score, final_result, confidence = self._calculate_final_score(
                result["scores"]["structural"],
                result["scores"]["semantic"],
                llm_score,
                llm_result["confidence"]
            )

            result["final_score"] = final_score
            result["final_result"] = final_result
            result["confidence"] = confidence

            # Step 6: Store results (done by caller)
            logger.info(f"Query {query_id} evaluation complete: {final_result} (score: {final_score:.2f})")

            return result

        except Exception as e:
            logger.error(f"Error evaluating query {query_id}: {e}")
            result["final_result"] = "ERROR"
            result["error"] = str(e)
            return result

    def _calculate_final_score(
        self,
        structural_score: float,
        semantic_score: float,
        llm_score: float,
        llm_confidence: float
    ) -> tuple:
        """
        Step 5: Calculate weighted final score

        Weights: 30% structural + 30% semantic + 40% LLM

        Args:
            structural_score: Structural validation score
            semantic_score: Semantic similarity score
            llm_score: LLM judge score
            llm_confidence: LLM confidence score

        Returns:
            (final_score, result, confidence)
        """
        # Weighted combination
        final_score = (
            0.30 * structural_score +
            0.30 * semantic_score +
            0.40 * llm_score
        )

        # Determine final result based on threshold
        threshold = settings.EVALUATION_THRESHOLD  # Default: 0.7
        final_result = "PASS" if final_score >= threshold else "FAIL"

        # Overall confidence (average of LLM confidence and score confidence)
        score_confidence = final_score  # Higher score = higher confidence
        confidence = (llm_confidence + score_confidence) / 2.0

        return final_score, final_result, confidence

    def store_result(self, evaluation_result: Dict) -> bool:
        """
        Step 6: Store evaluation result in database

        Args:
            evaluation_result: Complete evaluation result

        Returns:
            Success status
        """
        try:
            conn = self._get_db_connection()
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO monitoring.evaluations (
                    query_id, query_text, agent_type, complexity,
                    generated_sql, ground_truth_sql,
                    structural_score, semantic_score, llm_score,
                    final_score, result, confidence,
                    reasoning, evaluation_data, created_at
                )
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                evaluation_result["query_id"],
                evaluation_result["query_text"],
                evaluation_result["agent_type"],
                evaluation_result["complexity"],
                evaluation_result["generated_sql"],
                evaluation_result["ground_truth_sql"],
                evaluation_result["scores"].get("structural", 0.0),
                evaluation_result["scores"].get("semantic", 0.0),
                evaluation_result["scores"].get("llm", 0.0),
                evaluation_result["final_score"],
                evaluation_result["final_result"],
                evaluation_result["confidence"],
                evaluation_result["steps"].get("llm_judge", {}).get("reasoning", ""),
                json.dumps(evaluation_result["steps"]),
                datetime.now()
            ))

            conn.commit()
            cursor.close()
            conn.close()

            logger.debug(f"Stored evaluation result for {evaluation_result['query_id']}")
            return True

        except Exception as e:
            logger.error(f"Error storing evaluation result: {e}")
            return False
